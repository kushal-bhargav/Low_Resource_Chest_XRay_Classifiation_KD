# Knowledge Distillation for Low-Resolution Chest X-Ray Classification

## Abstract

## Methodology


## Literature Survey

## Application



## Identified Gaps


## TODO

## Contributions

## Running the Code

You can run the main pipeline using the following CLI command:

```bash
python main.py --config configs/distill_config.yaml
```

Ensure the config file specifies paths to datasets, model architectures, and training hyperparameters.

## Code Reference & Acknowledgments

This repository builds upon the excellent work in [abhiWriteCode/Knowledge-Distillation](https://github.com/abhiWriteCode/Knowledge-Distillation) and is inspired by:

> Akhter, Y., Ranjan, R., Singh, R., & Vatsa, M. (2024). Low-Resolution Chest X-Ray Classification Via Knowledge Distillation and Multi-Task Learning. *2024 IEEE ISBI*, pp. 1â€“5.


